{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08fd6de9d0bfe166eadfd2b806920a886e6c2c286fb4708dbd1d86a030e84ebcf",
   "display_name": "Python 3.7.10 64-bit ('wav2kp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Filipe\\.conda\\envs\\wav2kp\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data.base_data_module import BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([80, 1600, 721])\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = np.load(\"dataset/Train.npy\", allow_pickle=True)\n",
    "# test_dataset = np.load(\"dataset/Test.npy\", allow_pickle=True)\n",
    "# val_dataset = np.load(\"dataset/Val.npy\", allow_pickle=True)\n",
    "\n",
    "train_keypoints = np.load(\"dataset/Train_keypoints.npy\",  allow_pickle=True)\n",
    "test_keypoints = np.load(\"dataset/Test_keypoints.npy\",  allow_pickle=True)\n",
    "val_keypoints = np.load(\"dataset/Val_keypoints.npy\",  allow_pickle=True)\n",
    "\n",
    "train_mfccs = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "test_mfccs = torch.load(\"dataset/Test_mfccs.pt\" )\n",
    "val_mfccs = torch.load(\"dataset/Val_mfccs.pt\" )\n",
    "\n",
    "train_mfccs_not_transp = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "train_mfccs_not_transp = torch.transpose(train_mfccs_not_transp, 2, 1)\n",
    "\n",
    "train_set  = BaseDataset(data=train_mfccs, targets=train_keypoints)\n",
    "\n",
    "print(train_mfccs_not_transp.shape)\n",
    "train_set  = BaseDataset(data=train_mfccs_not_transp, targets=train_keypoints)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set)\n",
    "# test_loader = DataLoader(test_dataset)\n",
    "# val_loader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_keypoints = np.load(\"dataset/Train_keypoints.npy\",  allow_pickle=True)\n",
    "\n",
    "\n",
    "train_mfccs = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "\n",
    "\n",
    "train_mfccs_not_transp = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "train_mfccs_not_transp = torch.transpose(train_mfccs_not_transp, 2, 1)\n",
    "\n",
    "train_set  = BaseDataset(data=train_mfccs, targets=train_keypoints)\n",
    "train_set_nt  = BaseDataset(data=train_mfccs_not_transp, targets=train_keypoints)\n",
    "\n",
    "train_loader = DataLoader(train_set)\n",
    "train_loader_nt = DataLoader(train_set_nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input_size = train_set[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "# Here we define our model as a class\n",
    "class LSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2, dnn_shape=136):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dnn_shape = dnn_shape\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.dnn_shape, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        #print(f\"input.view(len(input) {input.view(len(input), self.batch_size, -1).shape}\")\n",
    "        # lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        lstm_out, self.hidden = self.lstm(input)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        # y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        #print(f\"y_pred.shape in net: {y_pred.shape}\")\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        # scheduler = StepLR(optimizer, step_size=800, gamma=0.5)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        #print(f\" X_batch:{X_batch.shape}, y_batch: {y_batch.shape}\")\n",
    "        y_pred = self.forward(X_batch)\n",
    "        #print(f\"y_pred.shape: {y_pred.shape}, y_batch.shape(): {y_batch.shape}\")\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss = loss_fn(y_pred, y_batch.float())\n",
    "        # criterion = nn.BCEWithLogitsLoss()\n",
    "        # loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X_batch, y_batch = batch\n",
    "        y_pred = self.forward(X_batch)\n",
    "        \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        # scheduler = StepLR(optimizer, step_size=800, gamma=0.5)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "model = LSTM(1600, 136, batch_size=1, output_dim=136, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "t/s, loss=0.00411, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  49%|████▉     | 39/80 [00:01<00:02, 19.64it/s, loss=0.00382, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  50%|█████     | 40/80 [00:02<00:02, 19.69it/s, loss=0.00364, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  51%|█████▏    | 41/80 [00:02<00:01, 19.79it/s, loss=0.00342, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  52%|█████▎    | 42/80 [00:02<00:01, 19.91it/s, loss=0.00308, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  54%|█████▍    | 43/80 [00:02<00:01, 19.98it/s, loss=0.00289, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  55%|█████▌    | 44/80 [00:02<00:01, 20.03it/s, loss=0.00272, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  56%|█████▋    | 45/80 [00:02<00:01, 20.10it/s, loss=0.00262, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  57%|█████▊    | 46/80 [00:02<00:01, 20.15it/s, loss=0.00246, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  59%|█████▉    | 47/80 [00:02<00:01, 20.21it/s, loss=0.00223, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  60%|██████    | 48/80 [00:02<00:01, 20.29it/s, loss=0.00208, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  61%|██████▏   | 49/80 [00:02<00:01, 20.37it/s, loss=0.00176, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  62%|██████▎   | 50/80 [00:02<00:01, 20.43it/s, loss=0.00158, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  64%|██████▍   | 51/80 [00:02<00:01, 20.52it/s, loss=0.00144, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  65%|██████▌   | 52/80 [00:02<00:01, 20.57it/s, loss=0.00133, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  66%|██████▋   | 53/80 [00:02<00:01, 20.63it/s, loss=0.00119, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  68%|██████▊   | 54/80 [00:02<00:01, 20.71it/s, loss=0.00109, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  69%|██████▉   | 55/80 [00:02<00:01, 20.78it/s, loss=0.00104, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  70%|███████   | 56/80 [00:02<00:01, 20.84it/s, loss=0.000975, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  71%|███████▏  | 57/80 [00:02<00:01, 20.90it/s, loss=0.000904, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  72%|███████▎  | 58/80 [00:02<00:01, 20.99it/s, loss=0.000806, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  74%|███████▍  | 59/80 [00:02<00:00, 21.05it/s, loss=0.000765, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  75%|███████▌  | 60/80 [00:02<00:00, 21.13it/s, loss=0.000747, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  76%|███████▋  | 61/80 [00:02<00:00, 21.17it/s, loss=0.000692, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  78%|███████▊  | 62/80 [00:02<00:00, 21.26it/s, loss=0.000674, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  79%|███████▉  | 63/80 [00:02<00:00, 21.33it/s, loss=0.000646, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  80%|████████  | 64/80 [00:02<00:00, 21.37it/s, loss=0.000625, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  81%|████████▏ | 65/80 [00:03<00:00, 21.44it/s, loss=0.000473, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  82%|████████▎ | 66/80 [00:03<00:00, 21.51it/s, loss=0.000429, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  84%|████████▍ | 67/80 [00:03<00:00, 21.53it/s, loss=0.000383, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  85%|████████▌ | 68/80 [00:03<00:00, 21.59it/s, loss=0.000326, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  86%|████████▋ | 69/80 [00:03<00:00, 21.64it/s, loss=0.000307, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  88%|████████▊ | 70/80 [00:03<00:00, 21.69it/s, loss=0.000289, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  89%|████████▉ | 71/80 [00:03<00:00, 21.72it/s, loss=0.000278, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  90%|█████████ | 72/80 [00:03<00:00, 21.76it/s, loss=0.000264, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  91%|█████████▏| 73/80 [00:03<00:00, 21.78it/s, loss=0.000256, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  92%|█████████▎| 74/80 [00:03<00:00, 21.81it/s, loss=0.000251, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  94%|█████████▍| 75/80 [00:03<00:00, 21.83it/s, loss=0.00022, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  95%|█████████▌| 76/80 [00:03<00:00, 21.87it/s, loss=0.000191, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  96%|█████████▋| 77/80 [00:03<00:00, 21.91it/s, loss=0.000177, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  98%|█████████▊| 78/80 [00:03<00:00, 21.95it/s, loss=0.000164, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 0:  99%|█████████▉| 79/80 [00:03<00:00, 21.97it/s, loss=0.000155, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   0%|          | 0/80 [00:00<?, ?it/s, loss=0.000148, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   1%|▏         | 1/80 [00:00<00:03, 25.64it/s, loss=0.000144, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   2%|▎         | 2/80 [00:00<00:03, 25.32it/s, loss=0.000135, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   4%|▍         | 3/80 [00:00<00:03, 25.42it/s, loss=0.00014, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   5%|▌         | 4/80 [00:00<00:03, 24.69it/s, loss=0.000135, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   6%|▋         | 5/80 [00:00<00:03, 24.51it/s, loss=0.000131, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   8%|▊         | 6/80 [00:00<00:03, 24.59it/s, loss=0.00013, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:   9%|▉         | 7/80 [00:00<00:02, 24.38it/s, loss=0.000127, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  10%|█         | 8/80 [00:00<00:02, 24.16it/s, loss=0.000127, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  11%|█▏        | 9/80 [00:00<00:02, 23.86it/s, loss=0.000123, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  12%|█▎        | 10/80 [00:00<00:02, 23.91it/s, loss=0.000121, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  14%|█▍        | 11/80 [00:00<00:02, 23.80it/s, loss=0.000111, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  15%|█▌        | 12/80 [00:00<00:02, 23.80it/s, loss=0.000109, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  16%|█▋        | 13/80 [00:00<00:02, 23.63it/s, loss=0.000109, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  18%|█▊        | 14/80 [00:00<00:02, 23.46it/s, loss=0.000104, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  19%|█▉        | 15/80 [00:00<00:02, 23.75it/s, loss=0.000106, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  20%|██        | 16/80 [00:00<00:02, 24.00it/s, loss=0.000107, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  21%|██▏       | 17/80 [00:00<00:02, 24.06it/s, loss=0.000107, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  22%|██▎       | 18/80 [00:00<00:02, 24.20it/s, loss=0.000106, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  24%|██▍       | 19/80 [00:00<00:02, 24.25it/s, loss=0.000104, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  25%|██▌       | 20/80 [00:00<00:02, 24.08it/s, loss=0.0001, v_num=60]  y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  26%|██▋       | 21/80 [00:00<00:02, 24.09it/s, loss=9.94e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  28%|██▊       | 22/80 [00:00<00:02, 24.13it/s, loss=0.000107, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  29%|██▉       | 23/80 [00:00<00:02, 24.27it/s, loss=9.72e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  30%|███       | 24/80 [00:00<00:02, 24.25it/s, loss=9.45e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  31%|███▏      | 25/80 [00:01<00:02, 24.23it/s, loss=9.73e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  32%|███▎      | 26/80 [00:01<00:02, 24.33it/s, loss=9.86e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  34%|███▍      | 27/80 [00:01<00:02, 24.40it/s, loss=9.58e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  35%|███▌      | 28/80 [00:01<00:02, 24.50it/s, loss=9.12e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  36%|███▋      | 29/80 [00:01<00:02, 24.56it/s, loss=0.00011, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  38%|███▊      | 30/80 [00:01<00:02, 24.68it/s, loss=0.000115, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  39%|███▉      | 31/80 [00:01<00:01, 24.85it/s, loss=0.000114, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  40%|████      | 32/80 [00:01<00:01, 24.99it/s, loss=0.000111, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  41%|████▏     | 33/80 [00:01<00:01, 25.12it/s, loss=0.000119, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  42%|████▎     | 34/80 [00:01<00:01, 25.23it/s, loss=0.000127, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  44%|████▍     | 35/80 [00:01<00:01, 25.29it/s, loss=0.000128, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  45%|████▌     | 36/80 [00:01<00:01, 25.29it/s, loss=0.000125, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  46%|████▋     | 37/80 [00:01<00:01, 25.35it/s, loss=0.000128, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  48%|████▊     | 38/80 [00:01<00:01, 25.34it/s, loss=0.000136, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  49%|████▉     | 39/80 [00:01<00:01, 25.35it/s, loss=0.000134, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  50%|█████     | 40/80 [00:01<00:01, 25.40it/s, loss=0.000133, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  51%|█████▏    | 41/80 [00:01<00:01, 25.45it/s, loss=0.000138, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  52%|█████▎    | 42/80 [00:01<00:01, 25.50it/s, loss=0.00013, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  54%|█████▍    | 43/80 [00:01<00:01, 25.42it/s, loss=0.000131, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  55%|█████▌    | 44/80 [00:01<00:01, 25.44it/s, loss=0.000133, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  56%|█████▋    | 45/80 [00:01<00:01, 25.40it/s, loss=0.000133, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  57%|█████▊    | 46/80 [00:01<00:01, 25.37it/s, loss=0.000131, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  59%|█████▉    | 47/80 [00:01<00:01, 25.36it/s, loss=0.000131, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  60%|██████    | 48/80 [00:01<00:01, 25.41it/s, loss=0.000147, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  61%|██████▏   | 49/80 [00:01<00:01, 25.44it/s, loss=0.00013, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  62%|██████▎   | 50/80 [00:01<00:01, 25.48it/s, loss=0.000126, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  64%|██████▍   | 51/80 [00:01<00:01, 25.56it/s, loss=0.000128, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  65%|██████▌   | 52/80 [00:02<00:01, 25.60it/s, loss=0.000132, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  66%|██████▋   | 53/80 [00:02<00:01, 25.65it/s, loss=0.000122, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  68%|██████▊   | 54/80 [00:02<00:01, 25.71it/s, loss=0.000114, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  69%|██████▉   | 55/80 [00:02<00:00, 25.73it/s, loss=0.000124, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  70%|███████   | 56/80 [00:02<00:00, 25.76it/s, loss=0.000128, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  71%|███████▏  | 57/80 [00:02<00:00, 25.78it/s, loss=0.000126, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  72%|███████▎  | 58/80 [00:02<00:00, 25.78it/s, loss=0.000118, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  74%|███████▍  | 59/80 [00:02<00:00, 25.76it/s, loss=0.000119, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  75%|███████▌  | 60/80 [00:02<00:00, 25.76it/s, loss=0.000122, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  76%|███████▋  | 61/80 [00:02<00:00, 25.81it/s, loss=0.000115, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  78%|███████▊  | 62/80 [00:02<00:00, 25.87it/s, loss=0.000115, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  79%|███████▉  | 63/80 [00:02<00:00, 25.83it/s, loss=0.000114, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  80%|████████  | 64/80 [00:02<00:00, 25.81it/s, loss=0.000115, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  81%|████████▏ | 65/80 [00:02<00:00, 25.81it/s, loss=0.00011, v_num=60] y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  82%|████████▎ | 66/80 [00:02<00:00, 25.77it/s, loss=0.000108, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  84%|████████▍ | 67/80 [00:02<00:00, 25.79it/s, loss=0.000108, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  85%|████████▌ | 68/80 [00:02<00:00, 25.72it/s, loss=9.29e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  86%|████████▋ | 69/80 [00:02<00:00, 25.78it/s, loss=9.15e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  88%|████████▊ | 70/80 [00:02<00:00, 25.79it/s, loss=9.05e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  89%|████████▉ | 71/80 [00:02<00:00, 25.82it/s, loss=9.68e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  90%|█████████ | 72/80 [00:02<00:00, 25.84it/s, loss=9.67e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  91%|█████████▏| 73/80 [00:02<00:00, 25.85it/s, loss=9.63e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  92%|█████████▎| 74/80 [00:02<00:00, 25.82it/s, loss=9.84e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  94%|█████████▍| 75/80 [00:02<00:00, 25.88it/s, loss=8.54e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  95%|█████████▌| 76/80 [00:02<00:00, 25.91it/s, loss=8.03e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  96%|█████████▋| 77/80 [00:02<00:00, 25.93it/s, loss=7.94e-05, v_num=60]y_pred.shape: torch.Size([1, 721, 136]), y_batch.shape(): torch.Size([1, 721, 136])\n",
      "Epoch 1:  98%|█████████▊| 78/80 [00:03<00:00, 25.62it/s, loss=7.94e-05, v_num=60]\n",
      "C:\\Users\\Filipe\\.conda\\envs\\wav2kp\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if True:\n",
    "    logger = pl.loggers.TensorBoardLogger(\"training/logs\")\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(logger=logger, weights_save_path=\"training/logs\", gpus=1 )\n",
    "\n",
    "    trainer.fit(model, train_loader)\n",
    "    # trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aaaaa\n",
    "\n",
    "train_keypoints = np.load(\"dataset/Train_keypoints.npy\",  allow_pickle=True)\n",
    "\n",
    "\n",
    "train_mfccs = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "\n",
    "\n",
    "train_mfccs_not_transp = torch.load(\"dataset/Train_mfccs.pt\" )\n",
    "train_mfccs_not_transp = torch.transpose(train_mfccs_not_transp, 2, 1)\n",
    "\n",
    "train_set  = BaseDataset(data=train_mfccs, targets=train_keypoints)\n",
    "train_set_nt  = BaseDataset(data=train_mfccs_not_transp, targets=train_keypoints)\n",
    "\n",
    "train_loader = DataLoader(train_set)\n",
    "train_loader_nt = DataLoader(train_set_nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 721, 1600]) 1600\ntorch.Size([1, 721, 136])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "first_mfcc, first_kp = next(it)\n",
    "print( first_mfcc.shape, first_mfcc.size(-1))\n",
    "print( first_kp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.2578, 0.5078, 0.2578, 0.5664, 0.2695, 0.6211, 0.2773, 0.6719, 0.2930,\n",
       "        0.7148, 0.3320, 0.7617, 0.3594, 0.7812, 0.4141, 0.8047, 0.4805, 0.8281,\n",
       "        0.5547, 0.8203, 0.5977, 0.7969, 0.6328, 0.7812, 0.6719, 0.7461, 0.6992,\n",
       "        0.7031, 0.7148, 0.6484, 0.7383, 0.5977, 0.7539, 0.5391, 0.3164, 0.4805,\n",
       "        0.3516, 0.4727, 0.3906, 0.4727, 0.4258, 0.4805, 0.4570, 0.4883, 0.5742,\n",
       "        0.4922, 0.6055, 0.4883, 0.6406, 0.4883, 0.6797, 0.4883, 0.7070, 0.5000,\n",
       "        0.5078, 0.5469, 0.5078, 0.5820, 0.5078, 0.6133, 0.5000, 0.6406, 0.4570,\n",
       "        0.6484, 0.4805, 0.6562, 0.5000, 0.6641, 0.5234, 0.6641, 0.5391, 0.6562,\n",
       "        0.3594, 0.5312, 0.3828, 0.5234, 0.4141, 0.5234, 0.4414, 0.5391, 0.4180,\n",
       "        0.5469, 0.3828, 0.5469, 0.5742, 0.5469, 0.6055, 0.5391, 0.6328, 0.5391,\n",
       "        0.6562, 0.5469, 0.6328, 0.5586, 0.6055, 0.5547, 0.4062, 0.7070, 0.4336,\n",
       "        0.7031, 0.4805, 0.7031, 0.4922, 0.7031, 0.5156, 0.7031, 0.5547, 0.7070,\n",
       "        0.5742, 0.7148, 0.5469, 0.7305, 0.5234, 0.7305, 0.4922, 0.7305, 0.4648,\n",
       "        0.7305, 0.4414, 0.7227, 0.4141, 0.7070, 0.4727, 0.7148, 0.4922, 0.7148,\n",
       "        0.5234, 0.7148, 0.5742, 0.7148, 0.5234, 0.7070, 0.4922, 0.7070, 0.4727,\n",
       "        0.7070])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "first_kp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.2578) tensor(0.7070)\ntensor(0.2578) tensor(0.7070)\n"
     ]
    }
   ],
   "source": [
    "print(torch.reshape(first_kp[0,0], (1, 136)).squeeze()[0], torch.reshape(first_kp[0,0], (1, 136)).squeeze()[-1])\n",
    "print(first_kp[0,0, 0],first_kp[0,0, -1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(1600, 136, 1, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 721, 1600]) 1600\n"
     ]
    }
   ],
   "source": [
    "print( first_mfcc.shape, first_mfcc.size(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hn, cn) = lstm(first_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 721, 136])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =  nn.Linear(136, 136)\n",
    "\n",
    "y_pred = dl(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.3615,  0.1972, -0.2867,  ...,  0.2887, -0.2519, -0.3320],\n",
       "         [-0.4358,  0.2319, -0.4761,  ...,  0.4729,  0.1836, -0.2334],\n",
       "         [-0.5979,  0.0870, -0.4274,  ...,  0.5176,  0.0870, -0.2092],\n",
       "         ...,\n",
       "         [ 0.0095, -0.0070, -0.0296,  ...,  0.0615, -0.0526,  0.0062],\n",
       "         [ 0.0095, -0.0070, -0.0296,  ...,  0.0615, -0.0526,  0.0062],\n",
       "         [ 0.0095, -0.0070, -0.0296,  ...,  0.0615, -0.0526,  0.0062]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}