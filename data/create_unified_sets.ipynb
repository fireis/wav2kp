{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08fd6de9d0bfe166eadfd2b806920a886e6c2c286fb4708dbd1d86a030e84ebcf",
   "display_name": "Python 3.7.10 64-bit ('wav2kp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Filipe\\.conda\\envs\\wav2kp\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchaudio import transforms\n",
    "import torch\n",
    "from typing import Any, Callable, Dict, Sequence, Tuple, Union\n",
    "SequenceOrTensor = Union[Sequence, torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # TODO: MAKE THIS READ TEH PATH FROM ARGPARSE\n",
    "    file_path = os.path.abspath(\"\").replace(\"data\", \"docs\") + \"/train_val_test_dist.xlsx\"\n",
    "    train_test_dist = pd.read_excel(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = train_test_dist[train_test_dist.set == \"Train\"].video.unique()\n",
    "test_files = train_test_dist[train_test_dist.set == \"Test\"].video.unique()\n",
    "val_files = train_test_dist[train_test_dist.set == \"Val\"].video.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 80/80 [00:25<00:00,  3.13it/s]\n",
      "C:\\Users\\Filipe\\.conda\\envs\\wav2kp\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.08it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def assemble_path(file_name):\n",
    "    file_path = os.path.abspath(\"\").replace(\"data\", \"raw_videos/\") + file_name + \"/\"\n",
    "    audio_file = file_path + \"audio/\" + file_name + \".wav\"\n",
    "    keypoints_folder = file_path + \"keypoints_reduced/\" \n",
    "\n",
    "    return audio_file, keypoints_folder\n",
    "\n",
    "def find_keypoints(keypoints_folder):\n",
    "    keypoints = []\n",
    "    # open the files while sorting as the order matters\n",
    "    frames_list = sorted(glob.glob(keypoints_folder + \"*.txt\"))\n",
    "    # read each txt with the samples\n",
    "    for frame in frames_list:\n",
    "        keypoints.append(read_keypoint_txt(frame))\n",
    "    \n",
    "    # transform to numpy array to make processing easier\n",
    "    keypoints = np.array(keypoints) \n",
    "\n",
    "    return keypoints\n",
    "\n",
    "def read_keypoint_txt(keypoints_folder):\n",
    "    keypoints = np.loadtxt(keypoints_folder, delimiter=\",\")\n",
    "    return keypoints\n",
    "\n",
    "def read_audio_file(path=\"\"):\n",
    "    \"\"\"\n",
    "    Reads the specifies file in path and returns it in Tensor format\n",
    "    \"\"\"\n",
    "    # fix to work on windows\n",
    "    path = path.replace(\"\\\\\", \"\\\\\")\n",
    "    return torchaudio.load(path)\n",
    "\n",
    "def extract_mfcc(audio_file_path):\n",
    "    audio, sr =  read_audio_file(audio_file_path)\n",
    "    mfcc = transforms.MFCC(sample_rate=sr, melkwargs={\"n_mels\": 40})\n",
    "    coefs = mfcc(audio)\n",
    "    coefs = torch.transpose(coefs, 1, 2)\n",
    "    coefs = torch.squeeze(coefs)\n",
    "    return coefs\n",
    "\n",
    "def assemble_set(train_test_dist, set_name=\"Train\"):\n",
    "    file_names = train_test_dist[train_test_dist.set == set_name].video.unique()\n",
    "    mfccs_list = []\n",
    "    keypoints_list = []\n",
    "    ds = []\n",
    "    file_names = tqdm(file_names)\n",
    "    for file_name in file_names:\n",
    "        # get  the files to process\n",
    "        audio_file_path, keypoints_folder = assemble_path(file_name)\n",
    "        # extract keypoints\n",
    "        keypoints = find_keypoints(keypoints_folder)\n",
    "        # extract mfccs\n",
    "        mfccs = extract_mfcc(audio_file_path)\n",
    "        # append to the dataset list\n",
    "        mfccs_list.append(mfccs)\n",
    "        keypoints_list.append(keypoints)\n",
    "    return mfccs_list, keypoints_list\n",
    "\n",
    "for set_dist in [\"Train\", \"Val\", \"Test\"]:\n",
    "    dataset_folder = os.path.abspath(\"\").replace(\"data\", \"dataset/\")\n",
    "\n",
    "    mfccs, keypoints = assemble_set(train_test_dist, set_name=set_dist)\n",
    "    mfccs = pad_sequence(mfccs, batch_first=True)\n",
    "    torch.save(mfccs, f\"{dataset_folder}{set_dist}_mfccs.pt\" )\n",
    "    np.save(f\"{dataset_folder}{set_dist}_keypoints\", keypoints, allow_pickle=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#pad_sequence(mfccs, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4983, 40])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "mfccs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([21, 4983, 40])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_keypoints(keypoints, max_size=(256, 256)):\n",
    "    # divide all x and y coordinates by the corresponding axis max\n",
    "    # this is defined on the image processing stage\n",
    "    keypoints[:, :, 0] = keypoints[:, :, 0]/max_size[0]\n",
    "    keypoints[:, :, 1] = keypoints[:, :, 1]/max_size[1]\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Base Dataset class that simply processes data and targets through optional transforms.\n",
    "\n",
    "    Read more: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        commonly these are torch tensors, numpy arrays, or PIL Images\n",
    "    targets\n",
    "        commonly these are torch tensors or numpy arrays\n",
    "    transform\n",
    "        function that takes a datum and returns the same\n",
    "    target_transform\n",
    "        function that takes a target and returns the same\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: SequenceOrTensor,\n",
    "        targets: SequenceOrTensor,\n",
    "        transform: Callable = None,\n",
    "        target_transform: Callable = None,\n",
    "        \n",
    "    ) -> None:\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError(\"Data and targets must be of equal length\")\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Return a datum and its target, after processing by transforms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (datum, target)\n",
    "        \"\"\"\n",
    "        datum, target = self.data[index], self.targets[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            datum = self.transform(datum)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return datum, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'asdsadfdaasdsad' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2887b2890752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0masdsadfdaasdsad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'asdsadfdaasdsad' is not defined"
     ]
    }
   ],
   "source": [
    "asdsadfdaasdsad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = np.load(f\"{dataset_folder}{set_dist}_keypoints.npy\",  allow_pickle=True)\n",
    "mfccs = torch.load(f\"{dataset_folder}{set_dist}_mfccs.pt\" )\n",
    "\n",
    "train_set  = BaseDataset(data=mfccs, targets=keypoints, target_transform=scale_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.26171875, 0.5078125 ],\n",
       "        [0.26171875, 0.56640625],\n",
       "        [0.27734375, 0.6171875 ],\n",
       "        ...,\n",
       "        [0.52734375, 0.70703125],\n",
       "        [0.50390625, 0.70703125],\n",
       "        [0.48046875, 0.70703125]],\n",
       "\n",
       "       [[0.26171875, 0.5078125 ],\n",
       "        [0.26953125, 0.56640625],\n",
       "        [0.2734375 , 0.6171875 ],\n",
       "        ...,\n",
       "        [0.52734375, 0.70703125],\n",
       "        [0.50390625, 0.70703125],\n",
       "        [0.48046875, 0.69921875]],\n",
       "\n",
       "       [[0.2578125 , 0.5078125 ],\n",
       "        [0.2578125 , 0.56640625],\n",
       "        [0.26953125, 0.6171875 ],\n",
       "        ...,\n",
       "        [0.5234375 , 0.70703125],\n",
       "        [0.5       , 0.70703125],\n",
       "        [0.4765625 , 0.70703125]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.25390625, 0.5078125 ],\n",
       "        [0.26171875, 0.56640625],\n",
       "        [0.26953125, 0.6171875 ],\n",
       "        ...,\n",
       "        [0.52734375, 0.70703125],\n",
       "        [0.5       , 0.69921875],\n",
       "        [0.4765625 , 0.69921875]],\n",
       "\n",
       "       [[0.25390625, 0.5078125 ],\n",
       "        [0.26171875, 0.56640625],\n",
       "        [0.26953125, 0.62890625],\n",
       "        ...,\n",
       "        [0.52734375, 0.703125  ],\n",
       "        [0.5078125 , 0.703125  ],\n",
       "        [0.4765625 , 0.703125  ]],\n",
       "\n",
       "       [[0.25390625, 0.515625  ],\n",
       "        [0.26171875, 0.57421875],\n",
       "        [0.265625  , 0.625     ],\n",
       "        ...,\n",
       "        [0.52734375, 0.69921875],\n",
       "        [0.49609375, 0.69921875],\n",
       "        [0.4765625 , 0.69921875]]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "aud, kp = train_set[0]\n",
    "kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 1947])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}